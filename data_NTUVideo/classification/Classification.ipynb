{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary classification is one of the most fundamental problem in machine learning. In this tutorial, you are going to build linear binary classifiers to predict whether the income of an indivisual exceeds 50,000 or not. We presented a discriminative and a generative approaches, the logistic regression(LR) and the linear discriminant anaysis(LDA). You are encouraged to compare the differences between the two, or explore more methodologies. Although you can finish this tutorial by simpliy copying and pasting the codes, we strongly recommend you to understand the mathematical formulation first to get more insight into the two algorithms. Please find [here](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Logistic%20Regression%20(v3).pdf) and [here](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Classification%20(v3).pdf) for more detailed information about the two algorithms.\n",
    "\n",
    "二元分類是機器學習中最基礎的問題之一，在這份教學中，你將學會如何實作一個線性二元分類器，來根據人們的個人資料，判斷其年收入是否高於 50,000 美元。我們將以兩種方法: logistic regression 與 generative model，來達成以上目的，你可以嘗試了解、分析兩者的設計理念及差別。針對這兩個演算法的理論基礎，可以參考李宏毅老師的教學投影片 [logistic regression](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Logistic%20Regression%20(v3).pdf) 與 [generative model](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Classification%20(v3).pdf)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "This dataset is obtained by removing unnecessary attributes and balancing the ratio between positively and negatively labeled data in the Census-Income (KDD) Data Set, which can be found in UCI Machine Learning Repository. Only preprocessed and one-hot encoded data (i.e. X_train, Y_train and X_test) will be used in this tutorial. Raw data (i.e. train.csv and test.csv) are provided to you in case you are interested in it.\n",
    "\n",
    "這個資料集是由 UCI Machine Learning Repository 的 Census-Income (KDD) Data Set 經過一些處理而得來。為了方便訓練，我們移除了一些不必要的資訊，並且稍微平衡了正負兩種標記的比例。事實上在訓練過程中，只有 X_train、Y_train 和 X_test 這三個經過處理的檔案會被使用到，train.csv 和 test.csv 這兩個原始資料檔則可以提供你一些額外的資訊。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In this section we will introduce logistic regression first. We only present how to implement it here, while mathematical formulation and analysis will be omitted. You can find more theoretical detail in [Prof. Lee's lecture](https://www.youtube.com/watch?v=hSXFuypLukA).\n",
    "\n",
    "首先我們會實作 logistic regression，針對理論細節說明請參考[李宏毅老師的教學影片](https://www.youtube.com/watch?v=hSXFuypLukA)\n",
    "\n",
    "###Preparing Data\n",
    "\n",
    "Load and normalize data, and then split training data into training set and development set.\n",
    "\n",
    "下載資料，並且對每個屬性做正規化，處理過後再將其切分為訓練集與發展集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 5425\n",
      "Size of development set: 48831\n",
      "Size of testing set: 27622\n",
      "Dimension of data: 510\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X_train_fpath = './data/X_train'\n",
    "Y_train_fpath = './data/Y_train'\n",
    "X_test_fpath = './data/X_test'\n",
    "output_fpath = './output_{}.csv'\n",
    "\n",
    "# Parse csv files to numpy array\n",
    "\n",
    "with open(X_train_fpath) as f:\n",
    "    next(f)\n",
    "    X_train = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\n",
    "    # line.strip('\\n') 切除前後的'\\n'\n",
    "with open(Y_train_fpath) as f:\n",
    "    next(f)\n",
    "    Y_train = np.array([line.strip('\\n').split(',')[1] for line in f], dtype = float)\n",
    "    # line.strip('\\n') 切除前後的'\\n'\n",
    "with open(X_test_fpath) as f:\n",
    "    next(f)\n",
    "    X_test = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\n",
    "\n",
    "def _normalize(X, train = True, specified_column = None, X_mean = None, X_std = None):\n",
    "    # This function normalizes specific columns of X.\n",
    "    # The mean and standard variance of training data will be reused when processing testing data.\n",
    "    #\n",
    "    # Arguments:\n",
    "    #     X: data to be processed\n",
    "    #     train: 'True' when processing training data, 'False' for testing data\n",
    "    #     specific_column: indexes of the columns that will be normalized. If 'None', all columns\n",
    "    #         will be normalized.\n",
    "    #     X_mean: mean value of training data, used when train = 'False'\n",
    "    #     X_std: standard deviation of training data, used when train = 'False'\n",
    "    # Outputs:\n",
    "    #     X: normalized data\n",
    "    #     X_mean: computed mean value of training data\n",
    "    #     X_std: computed standard deviation of training data\n",
    "\n",
    "    if specified_column == None:\n",
    "        specified_column = np.arange(X.shape[1])\n",
    "        # 若沒有指定行數 ，則全部normalize ，取得 X 的行數\n",
    "    if train:\n",
    "        X_mean = np.mean(X[:, specified_column] ,0).reshape(1, -1)\n",
    "        # 這邊會直接將所有specified_column內的數字全部做一遍 ， 完成後即完成所有col的標準化(若傳入為none)\n",
    "        X_std  = np.std(X[:, specified_column], 0).reshape(1, -1)\n",
    "\n",
    "    X[:,specified_column] = (X[:, specified_column] - X_mean) / (X_std + 1e-8)\n",
    "     \n",
    "    return X, X_mean, X_std\n",
    "\n",
    "def _train_def_split(X , Y , devRatio = 0.25):\n",
    "    # This function spilts data into training set and development set.\n",
    "    train_size = int(len(X) * devRatio)\n",
    "    return X[:train_size] , Y[:train_size] , X[train_size:] , Y[train_size:]\n",
    "\n",
    "X_train , X_mean , X_std = _normalize(X_train , train = True)\n",
    "X_test , _ , _ = _normalize(X_test, train = False , X_mean = X_mean , X_std = X_std)\n",
    "\n",
    "# Split data into training set and development set\n",
    "dev_ratio = 0.1\n",
    "X_train, Y_train, X_dev, Y_dev = _train_def_split(X_train, Y_train, devRatio = dev_ratio)\n",
    "\n",
    "train_size = X_train.shape[0]\n",
    "dev_size = X_dev.shape[0]\n",
    "test_size = X_test.shape[0]\n",
    "data_dim = X_train.shape[1]\n",
    "print('Size of training set: {}'.format(train_size))\n",
    "print('Size of development set: {}'.format(dev_size))\n",
    "print('Size of testing set: {}'.format(test_size))\n",
    "print('Dimension of data: {}'.format(data_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45., 46., 47., 48., 49., 50., 51., 52., 53., 54.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arraytemp = np.arange(100).reshape(10,-1)\n",
    "col = np.arange(arraytemp.shape[1])\n",
    "mean = np.mean(arraytemp[:,col] , 0)\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Useful Functions\n",
    "\n",
    "Some functions that will be repeatedly used when iteratively updating the parameters.\n",
    "\n",
    "這幾個函數可能會在訓練迴圈中被重複使用到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle( X ,Y):\n",
    "    # This function shuffles two equal-length list/array, X and Y, together.\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "    return ( X[randomize] , Y[randomize] )\n",
    "def _sigmoid(z) :\n",
    "    # Sigmoid function can be used to calculate probability.\n",
    "    # To avoid overflow, minimum/maximum output value is set.\n",
    "    return  np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))\n",
    "def _f(X, w, b):\n",
    "    # This is the logistic regression function, parameterized by w and b\n",
    "    #\n",
    "    # Arguements:\n",
    "    #     X: input data, shape = [batch_size, data_dimension]\n",
    "    #     w: weight vector, shape = [data_dimension, ]\n",
    "    #     b: bias, scalar\n",
    "    # Output:\n",
    "    #     predicted probability of each row of X being positively labeled, shape = [batch_size, ]\n",
    "    return _sigmoid(np.matmul( X , w ) + b)\n",
    "def _predict(X, w, b):\n",
    "    # This function returns a truth value prediction for each row of X \n",
    "    # by rounding the result of logistic regression functio\n",
    "    return np.round(_f(X,w,b)).astype(np.int)\n",
    "\n",
    "def _accurancy(Y_pred , Y_label):\n",
    "    acc = 1 - np.mean(np.abs(Y_pred - Y_label))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions about gradient and loss\n",
    "\n",
    "Please refers to [Prof. Lee's lecture slides](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Logistic%20Regression%20(v3).pdf)(p.12) for the formula of gradient and loss computation.\n",
    "\n",
    "請參考[李宏毅老師上課投影片](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Logistic%20Regression%20(v3).pdf)第 12 頁的梯度及損失函數計算公式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cross_entropy_loss(Y_pred , Y_label):\n",
    "    # This function computes the cross entropy.\n",
    "    #\n",
    "    # Arguements:\n",
    "    #     y_pred: probabilistic predictions, float vector\n",
    "    #     Y_label: ground truth labels, bool vector\n",
    "    # Output:\n",
    "    #     cross entropy, scalar\n",
    "    cross_entropy = -np.dot(Y_label ,np.log(Y_pred)) - np.dot((1-Y_label) ,np.log(1 - Y_pred))\n",
    "    return cross_entropy\n",
    "\n",
    "def _gradient(X , Y_label , w , b):\n",
    "    # This function computes the gradient of cross entropy loss with respect to weight w and bias b.\n",
    "    Y_pred = _f(X,w,b)\n",
    "    pred_error = Y_label - Y_pred\n",
    "    w_grad = -np.num(pred_error * X.T , 1)\n",
    "    b_grad = -np.sum(pred_error)\n",
    "    return w_grad, b_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
